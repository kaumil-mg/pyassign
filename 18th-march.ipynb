{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. The Filter method in feature selection involves evaluating the intrinsic characteristics of the features independently of the learning algorithm. It works by ranking or scoring each feature based on certain statistical measures such as correlation, mutual information, or statistical tests like ANOVA. Features are then selected or eliminated based on these scores, without considering the predictive power of the features in conjunction with the model.\n",
    "\n",
    "Q2. The Wrapper method differs from the Filter method in that it evaluates the performance of the model with different subsets of features. It involves training the model iteratively on different combinations of features and selecting the subset that yields the best performance according to a predefined evaluation metric. Unlike the Filter method, the Wrapper method considers the interaction between features and the model's predictive performance.\n",
    "\n",
    "Q3. Some common techniques used in Embedded feature selection methods include Lasso (L1 regularization), Ridge (L2 regularization), Elastic Net, decision trees, and gradient boosting machines. These methods embed feature selection within the model training process by penalizing the coefficients or importance of irrelevant features, effectively selecting the most informative features during model training.\n",
    "\n",
    "Q4. Drawbacks of using the Filter method for feature selection include its inability to capture feature interactions, its reliance on predetermined statistical measures which may not always correlate with model performance, and its inability to adapt to the specific learning algorithm being used.\n",
    "\n",
    "Q5. The Filter method is preferred over the Wrapper method in situations where there are a large number of features and computational resources are limited. It is also useful when the dataset is small or when there is a need for a quick initial assessment of feature importance without extensive model training.\n",
    "\n",
    "Q6. To choose the most pertinent attributes for the customer churn predictive model using the Filter Method in the telecom company project, you would first calculate statistical measures such as correlation or mutual information between each feature and the target variable (customer churn). Features with high correlation or mutual information scores would be considered more pertinent and selected for the model.\n",
    "\n",
    "Q7. In the soccer match outcome prediction project, you would use the Embedded method to select the most relevant features by employing techniques such as Lasso regularization or decision tree-based feature importance. These methods would penalize the coefficients or importance of irrelevant features during model training, effectively selecting the most informative features for predicting match outcomes.\n",
    "\n",
    "Q8. In the house price prediction project, you would use the Wrapper method to select the best set of features for the predictor by iteratively evaluating different combinations of features. This involves training the model with different subsets of features and selecting the subset that yields the best performance according to a predefined evaluation metric such as mean squared error or R-squared. This process helps ensure that only the most important features are included in the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
